---
published: false
title: Lecture 11. CNNs in Practice
category: [CS231n]
use_math: true
---

> 해당 포스트는 송교석 님의 유튜브 강의를 정리한 내용입니다. 강의 영상은 [여기](https://youtube.com/playlist?list=PL1Kb3QTCLIVtyOuMgyVgT-OeW0PYXl3j5)에서 보실 수 있습니다.

이번 포스트에서는 CNN 을 현실에서 어떻게 사용하는지에 대해서 깊이 있게 알아보도록 하겠습니다.

![p1](/images/cs231n/slides/lecture11/winter1516_lecture11-001.png)

![p1](/images/cs231n/slides/lecture11/winter1516_lecture11-010.png)

## Making the most of your data

### Data Augmentation

![p1](/images/cs231n/slides/lecture11/winter1516_lecture11-012.png)

CNN 에서는 Image 와 Label 을 CNN 에 feed 해주고, 이로부터 loss 를 계산해 줌으로써 오차를 줄여나가는 최적화 과정을 통해서 Classificaiton 을 하는 식으로 진행했었습니다.

![p1](/images/cs231n/slides/lecture11/winter1516_lecture11-013.png)

그런데, Data Augmentation 에서는 CNN 에 Image 를 feed 해주기 전에 데이터를 변형해주는 한 가지 과정을 더 거치게 됩니다.

그래서 어떠한 변형 방법을 이용할 것인지 정하는 것이 다양한 Augmentation 의 방법이 되겠습니다.

![p1](/images/cs231n/slides/lecture11/winter1516_lecture11-014.png)

Data Augmentation 이라는 것은 label 은 변경하지 않고 pixel 의 내용을 바꾸는 것입니다. 이렇게 변경된 데이터를 통해서 학습하고 실제로 매우 폭 넓게 이용되고 있습니다.

#### 1. Horizontal flips

![p1](/images/cs231n/slides/lecture11/winter1516_lecture11-015.png)

첫 번째로 가장 유명한 방법은 `Horizontal flip` 즉 좌우반전 입니다. 거울에 비춘것 처럼 변형이 되기 때문에 Mirror Image 라고도 합니다.

#### 2. Random crops/scales

![p1](/images/cs231n/slides/lecture11/winter1516_lecture11-018.png)

두 번째로, Random 한 crop 과 scale 이 되겠습니다. 이 방법은 먼저 위 그림처럼 랜덤하게 이미지를 잘라주고, scale 도 다양하게 합니다. 이렇게 random cropped/scaled Image 를 학습 시켜줍니다.

예를 들어 ResNet 에서는 [256, 480] 의 범위에서 랜덤하게 수를 선택해주고 training 이미지를 resize 해줍니다. 이때 짧은 부분이 L 이 되도록 하고, 이후 [224 x 224]의 패치를 랜덤하게 샘플링하여 추출해 줍니다. 이렇게 해서 나온 [224 x 224]의 이미지는 ResNet 에 들어가는 이미지의 크기가 되겠습니다.

이처럼 Augmentation 을 이용하면 Training 시에 이미지 전체가 아니라 Crop 된 패치에 대한 학습이 이루어지기 때문에 테스트시에도 이미지 전체가 아닌 정해진 수의 crop 을 이용해서 테스트를 진행하게 됩니다. 즉, 정해진 수의 crop 을 통해 평균내는 작업을 테스트시에 하게 되는 것 입니다.

예를 들어, 우측 이미지에서처럼 서로 다른 5개의 crop 을 가지고 있을 때, 이들을 horizental flip 을 하게 된다고 하면 총 10개의 crop 을 얻을 수 있는데, 이들을 평균내주는 작업을 하게 되다는 것 입니다.

![p1](/images/cs231n/slides/lecture11/winter1516_lecture11-019.png)

다시 ResNet 의 예를 들면, 


### Transfer learning
## All aobut convolutions
### How to arrange them
### How to compute them fast
## Implmentation details
### GPU/CPU, bottlenecks, distributed trainingb