---
published: false
title: Lecture 10. Recurrent Neural Networks
category: [CS231n]
use_math: true
---

> 해당 포스트는 송교석 님의 유튜브 강의를 정리한 내용입니다. 강의 영상은 [여기](https://youtube.com/playlist?list=PL1Kb3QTCLIVtyOuMgyVgT-OeW0PYXl3j5)에서 보실 수 있습니다.

이번 포스트에서는 Recurent Neural Network(RNN)에 대해서 알아보도록 하겠습니다.

![p1](/images/cs231n/slides/lecture10/winter1516_lecture10-01.png)

![p1](/images/cs231n/slides/lecture10/winter1516_lecture10-06.png)

일반적인 Neural Network 은 one to one 의 형태로 input layer, hidden layer, output layer 로 구성이 됩니다. input 레이어에는 일반적으로 fix 된 사이즈의 vector 이미지가 들어가고 hidden 레이어를 거쳐서 output 레이어로 나오게 되는데 output 레이어에서는 input 레이어에서와 마찬가지로 fix 된 사이즈를 갖는 output vector 를 도출해냅니다. 여기 output vector 는 class 의 score 가 됩니다.

그런데, RNN 에서는 이와는 조금 다른 형태로 input 이나 output 에 sequence 를 갖습니다. 위의 그림과 같이 one to many, many to one, many to many 의 형태를 갖는데 각각에 대해 알아보겠습니다.

![p1](/images/cs231n/slides/lecture10/winter1516_lecture10-07.png)

one to many 의 예시로는 Image Captioning 이 있습니다. 즉, 이 이미지를 설명하는 단어들의 sequence 를 출력해냅니다.

![p1](/images/cs231n/slides/lecture10/winter1516_lecture10-08.png)

many to one 의 예로는 Sentiment Classification 이 있습니다. 즉, 감정을 분류해내는 것으로 단어들로 구성된 sequence 가 있다고 할 때, 해당 sequence 의 감정이 positive 한지 negative 한지를 하나의 클래스로 분류하는 것이 되겠습니다.

![p1](/images/cs231n/slides/lecture10/winter1516_lecture10-09.png)

다음으로 many to many 의 예시로는 Machine Translation 이 되겠습니다. 예를 들어, 영어 단어로 구성된 문장이 들어왔을 떄, 한국어 단어로 구성된 문장으로 결과를 내놓게 되는 것입니다.

![p1](/images/cs231n/slides/lecture10/winter1516_lecture10-10.png)

또 다른 many to many 의 예시로는 Video Classification 이 있습니다. 비디오가 input 으로 들어오게 되면 frame level 에서 classify 하게 되는데, 여기서 중요한 것은 예측이 현재 시점에 국한된 함수면 안된다는 것입니다. 비디오에 있어서 예측이라는 것은 현재의 프레임과 prev, next 프레임을 포함한 모든 프레임에 대한 함수가 되어야 한다는 것입니다. 그래서 이처럼, RNN Architecture 의 핵심은 모든 각각의 timestep 에서의 현재의 frame + 지나간 frame 들에 대한 함수로 이루어지게 된다는 것입니다.

한 가지 더 짚고 넘어가자면, one to one 의 일반적인 neural network 의 경우 input 과 output 이 기본적인 sequence 의 형태는 아니지만, IO 의 fixed vector 를 하나의 sequence 로 간주할 수도 있다는 것입니다. 바로 이 예시가 뒤에 나옵니다.

![p1](/images/cs231n/slides/lecture10/winter1516_lecture10-11.png)

위의 예시는 fixed input 을 sequential 하게 처리한 경우로 Deep Mind 에서 ZIP code 를 인식하기 위해서 만든 모델입니다. 보시면 Convolutional Neural Network 에 큰 이미지를 feed 해서 번지수를 classify 하는 방식으로 하는 것이 아니라, RNN policy 를 이용했습니다. 작은 CNN 을 이용해서 이미지 전체를 sequential 하게 훑어나가는 식으로 진행됩니다.

![p1](/images/cs231n/slides/lecture10/winter1516_lecture10-12.png)

위의 예시는 DRAW 라는 것으로, 앞의 예시와는 반대로 fixed output 을 sequential 하게 처리한 것입니다.

즉, 이것은 output 이기 때문에 무언가를 출력하는 generative model 이라고 할 수 있겠습니다. 영상을 보시게 되면, 한번에 숫자를 보여주는 것이 아니라 써내려가는 모습을 볼 수 있는데 RNN 에서의 특징을 볼 수 있습니다.

이와 같이, one to one 의 경우에서도 일반적인 CNN 이 아니라 RNN 을 이용해서 분석할 수 있다는 것입니다.

## Recurrent Neural Network

![p1](/images/cs231n/slides/lecture10/winter1516_lecture10-13.png)

지금부터 RNN 의 과정을 상세하게 살펴보겠습니다.

RNN 이 있고, 시간에 따라서 input vector 를 받게 됩니다. 즉, 매 time step 마다 input vector 가 RNN 으로 입력되게 되는데, 이 RNN 은 내부적으로 어떤 상태(state)를 가집니다. 그리고 이 상태를 function 으로 변형해줄 수 있습니다. 어떤 function 이냐면 매 time-step 마다 input 을 받는 것에 대한 function 으로 만들어 줄 수 있는데, 물론 RNN 도 weight 로 구성이 되며, weight 들이 tunning 되어 가면서 RNN 이 진화하기 떄문에 새로운 input 이 들어올 때마다 새로운 반응을 보이게 됩니다.

![p1](/images/cs231n/slides/lecture10/winter1516_lecture10-14.png)

그 이후 특정 time step 에서의 vector 를 예측하길 원하는 것입니다.

![p1](/images/cs231n/slides/lecture10/winter1516_lecture10-15.png)

매 time step 마다 입력되는 vector x 에 대해서 $h_t = f_W(h_{t-1}, x_t)$ 와 같은 `recurrence function` 을 적용할 수 있게 됩니다. 이를 적용함으로써 sequence 를 처리해줄 수 있게 되는 것인데, state 의 update 는 다음과 같이 일어납니다.

새로운 상태(state)를 $h_t$ 라고 하고, $f_W$ 는 파라마터 $W$ 에 대한 function으로 함수의 인자로는 바로 직전의 hidden state 인 $h_{t-1}$ 과 현재 time step 에서의 input vector 인 $x_t$ 가 됩니다.

참고로, 여기에서 state 는 일반적으로 vector 의 collection 으로 표현되고, recurrence function 은 파라미터 W 에 대한 function 이기 때문에 W 를 변경하게 되면 RNN 도 다른 behavior 을 보이게 될 것입니다.

그러므로 RNN 이 우리가 원하는 특정 behavior 를 가질 수 있도록 Weight 값들을 학습시켜나가게 되는 것입니다.

![p1](/images/cs231n/slides/lecture10/winter1516_lecture10-16.png)

여기서 주의해야하는 것은 매 time step 마다 동일한 function 과 parameter set 들이 사용되어야 한다는 것입니다. 이렇게함으로써 RNN 이 input과 output 의 sequence size 에 무관하게 적용이 가능하게 됩니다.

다시말해, input 과 output 의 sequence size 가 아무리 크더라도 문제가 없다는 의미입니다.

이러한 recurrence function 을 적용한 가장 간단한 사례인 vanilla RNN 에 대해 알아보겠습니다.

![p1](/images/cs231n/slides/lecture10/winter1516_lecture10-17.png)

vanilla RNN 에서는 상태가 단일의 hidden vector `h` 로만 구성이 됩니다. 그래서 앞에서 본 대로 현재의 state 는 직전의 state 와 현재의 입력값의 function 으로 표현이 됩니다.

이를 실제의 vanilla state update 로 표현하게 되면 $h_t = \text{tanh}(W_{hh}h_{t-1} + W_{xh}x_t)$가 됩니다. 그래서 $x_t$ 같은 경우 weight 값이 x 에서 hidden layer 로 가는 $W_{xh}$ 의 영향을 받기 때문에 $W_{xh}x_t$가 되고, 직전의 상태 $h_{t-1}$의 경우 현재의 hidden layer 와 직전의 hidden layer 의 영향을 받기 때문에 $W_{hh}$ 를 곱해줍니다. 그리고 이들을 더해 tanh 를 통해 squash 해줌으로써 현재의 state 인 $h_t$를 얻어내게 됩니다. 즉, 현재의 state 라는 것은 history 와 새로운 입력값에 의해서 상태가 변화하게 됩니다.

![p1](/images/cs231n/slides/lecture10/winter1516_lecture10-18.png)

이해를 돕기 위해 Character-level language model example 이라는 더 구체적인 예시를 살펴보겠습니다.

여기서 학습시키려고 하는 단어는 "hello" 라는 단어로, 이 단어에는 h, e, l, o 라는 4가지의 character 가 들어가 있습니다.

학습을 위해 character 의 sequence 를 RNN 에 feeding 해주고, 매 순간의 time step 에서 RNN 에 다음 step 에 어떤 character 가 올 것인지를 예측하게 합니다.

예를 들어, h 를 넣으면 그 다음 step 으로는 e 가 나오게 된다는 것을 예측 하게 합니다. 이렇게 한 step 씩 진행하면서 전체적인 distribution 을 예측하는 것입니다.

![p1](/images/cs231n/slides/lecture10/winter1516_lecture10-19.png)

RNN 에 feeding 을 해줄 떄는 위의 input [h, e, l, o] 를 `one-hot encoding`을 합니다.

![p1](/images/cs231n/slides/lecture10/winter1516_lecture10-20.png)

hidden layer는 일단 임의의 값으로 초기화한뒤, 입력을 받고 hidden layer 를 통해 어떤 출력값을 도출합니다. 그리고 hidden layer 는 다음 layer 로 영향을 주게 됩니다. 이렇게 이동하는 것을 $W_{hh}$ 라고 하고, input layer 에서 hidden layer 로 이동하는 것을 $W_{xh}$ 라고 합니다. 

![p1](/images/cs231n/slides/lecture10/winter1516_lecture10-21.png)

output layer 는 input layer 와 마찬가지로, 4개의 character 에 대한 array 로 represent 한 결과를 도출합니다.

첫 번째 레이어의 경우, e 에 해당하는 값을 2.2 로 output 을 도출한 것이고, 뒷 단의 레이어들의 경우 l은 -1.0, 1.9, o는 2.2 로 예측한 것을 볼 수 있습니다.

정답에 해당하는 값은 e, l, l, o 인데 예측값은 o, o, l, o 가 나온 것을 확인 할 수 있습니다. 이런식으로 오차가 발생하는 것을 확인할 수 있는데, 이러한 오차를 이용하여 loss 를 구해 다시 아래쪽으로 backpropagation 을 해줍니다. 이를 통해서, gradient 를 수정해나가며 계속해서 학습을 진행하게 됩니다.

물론, 각각의 time step 에는 softmax classifier 가 존재합니다. 이를 이용하여 loss 를 계산하게 됩니다. 그리고 또 한가지 기억해야 하는 것은 W_xh, W_hh, W_hy 가 위 그림에서 여러번씩 화살표로 사용되고 있는데, 처음 RNN 을 들어가면서 모든 time step 에서 동일한 recurrence function 과 동일한 parameter 가 사용된다고 했습니다. 그래서 각각의 과정(화살표)에서 다른 W 가 사용되는 것이 아니라 동일한 것이 사용된다는 것을 반드시 기억해야 합니다.

이런식으로 처리함으로써 input 과 output 의 sequence에 상관없이 처리가 가능하다는 것입니다.

좀 더 이해를 높히기 위해 코드를 살펴보겠습니다.

![p1](/images/cs231n/slides/lecture10/winter1516_lecture10-22.png)

이런식으로 character-level 의 RNN 은 python 기반의 numpy 를 이용하여 불과 112줄의 코드로 구현이 가능한 것입니다.

이를 좀 더 자세히 들여다 보겠습니다.

![p1](/images/cs231n/slides/lecture10/winter1516_lecture10-23.png)

코드의 첫번째 부분인 Data I/O 부분을 보면, numpy 를 import 하고 data 를 불러와 유니크한 character 를 저장하게 됩니다.

여기서 중요한 부분은 char 와 index 를 연관시켜주었다는 것인데, `char_to_ix`와 `ix_to_char` 를 이용하여 mapping 시켜 주었다는 점 입니다.

![p1](/images/cs231n/slides/lecture10/winter151 6_lecture10-24.png)

Initialization 에서는 hyperparameter 를 설정하고 weight 와 bias 를 설정해줍니다.

![p1](/images/cs231n/slides/lecture10/winter1516_lecture10-26.png)

Main loop

![p1](/images/cs231n/slides/lecture10/winter1516_lecture10-27.png)

![p1](/images/cs231n/slides/lecture10/winter1516_lecture10-28.png)

![p1](/images/cs231n/slides/lecture10/winter1516_lecture10-29.png)

![p1](/images/cs231n/slides/lecture10/winter1516_lecture10-30.png)

Loss function

![p1](/images/cs231n/slides/lecture10/winter1516_lecture10-31.png)

softmax

![p1](/images/cs231n/slides/lecture10/winter1516_lecture10-32.png)

backprop

![p1](/images/cs231n/slides/lecture10/winter1516_lecture10-33.png)

sample

![p1](/images/cs231n/slides/lecture10/winter1516_lecture10-34.png)

이 정도의 간단한 코드로 많은 일을 할 수 있습니다.

하나의 예로, 굉장히 긴 문장을 RNN 에 넣어서 Regenerate 하는 것을 살펴보겠습니다.

![p1](/images/cs231n/slides/lecture10/winter1516_lecture10-35.png)

input 으로 셰익스피어의 작품을 넣어보게 되면 다음과 같이 됩니다.

![p1](/images/cs231n/slides/lecture10/winter1516_lecture10-36.png)

처음 step 에서는 알아볼 수 없는 문장이 나오지만, 학습을 반복하면서 그럴싸한 문장이 생성되는 것을 볼 수 있습니다.

![p1](/images/cs231n/slides/lecture10/winter1516_lecture10-37.png)

최종적으로는 위와 같은 문장을 만들었는데, 얼핏보면 굉장히 있어보이는 문장이 나오게 됩니다.

이 외에도 LATEX 나 LINUX 를 학습시켜서 다음과 같이 그럴싸한 결과물들을 뽑아내는 것을 볼 수 있습니다.

![p1](/images/cs231n/slides/lecture10/winter1516_lecture10-38.png)
![p1](/images/cs231n/slides/lecture10/winter1516_lecture10-39.png)
![p1](/images/cs231n/slides/lecture10/winter1516_lecture10-40.png)
![p1](/images/cs231n/slides/lecture10/winter1516_lecture10-41.png)
![p1](/images/cs231n/slides/lecture10/winter1516_lecture10-42.png)
![p1](/images/cs231n/slides/lecture10/winter1516_lecture10-43.png)
![p1](/images/cs231n/slides/lecture10/winter1516_lecture10-44.png)
![p1](/images/cs231n/slides/lecture10/winter1516_lecture10-45.png)

다른 실험으로 각 셀들이 어디를 관찰하고 있는 지를 찾아본 것이 있습니다. 결론적으로, RNN 에서 hidden state vector 내의 cell 을 들여다보니, 그 cell 이 어떤 면에 대해서 excited 된다는 것을 발견했습니다.

물론 모든 cell 이 이런 규칙을 갖고있다는 것은 아니지만, 약 5%의 cell 들이 그러하다는 것입니다.

![p1](/images/cs231n/slides/lecture10/winter1516_lecture10-46.png)

그 첫 번째 예로, quote detection cell 입니다. 여기서 주목할 점은 seq_length 를 100으로 줬음에도 불구하고 100자가 넘은 범위에 대해 quote 를 검출했다는 점입니다. 이에 대해서는 전 단계의 hidden state vector 의 상태를 넘겨주고, 100자를 넘어가지만 generalize 하는 방식으로 인식하는 것 같다고 설명합니다.

![p1](/images/cs231n/slides/lecture10/winter1516_lecture10-47.png)

위의 예시는 톨스토이의 전쟁과 평화를 학습시킨 것인데, 여기서 특정 셀은 하나의 line 에 대한 것만 tracking 하는 것을 확인할 수 있습니다.

![p1](/images/cs231n/slides/lecture10/winter1516_lecture10-48.png)

다음으로는 if statement 를 검출하는 cell 있고,

![p1](/images/cs231n/slides/lecture10/winter1516_lecture10-49.png)

quote 와 comment 를 검출하는 cell

![p1](/images/cs231n/slides/lecture10/winter1516_lecture10-50.png)

code 의 depth 를 검출하는 cell 등이 되겠습니다.

![p1](/images/cs231n/slides/lecture10/winter1516_lecture10-51.png)

RNN 을 잘 활용한 또 하나의 예시는 Image Captioning 이 되겠습니다.

![p1](/images/cs231n/slides/lecture10/winter1516_lecture10-52.png)

Image Captioning 은 두 개의 module 로 이루어집니다.

첫 번째는 Convolutional Neural Network, 두 번째는 Recurrent Neural Network 가 되겠습니다.

`CNN` 에서는 이미지를 처리해주고, `RNN`에서는 시퀀스를 처리해주게 됩니다. 즉, 위의 화살표대로 CNN 의 결과물을 다시 입력값으로 받아서 출력해주는 generative 한 프로세스가 되겠습니다.

이 단계를 상세하게 살펴보면 다음과 같습니다.

![p1](/images/cs231n/slides/lecture10/winter1516_lecture10-53.png)

먼저 테스트 이미지를 받고,

![p1](/images/cs231n/slides/lecture10/winter1516_lecture10-54.png)

테스트 이미지를 CNN 에 feed 해줍니다.

Conv. Net 으로는 VGG NET 을 활용하고 있는 예시 입니다.

![p1](/images/cs231n/slides/lecture10/winter1516_lecture10-55.png)

layer 의 끝을 보게 되면, FC 와 softmax 가 있는데 이 2개의 layer 를 없애버립니다.

이렇게 해서 이 부분을 RNN 의 앞부분이 되게 연결하는 것입니다.

