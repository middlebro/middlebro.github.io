---
published: false
title: Lecture 13. Segmentation and Attension
category: [CS231n]
use_math: true
---

> 해당 포스트는 송교석 님의 유튜브 강의를 정리한 내용입니다. 강의 영상은 [여기](https://youtube.com/playlist?list=PL1Kb3QTCLIVtyOuMgyVgT-OeW0PYXl3j5)에서 보실 수 있습니다.

이번 포스트에서는 CNN 을 현실에서 어떻게 사용하는지에 대해서 깊이 있게 알아보도록 하겠습니다.

![p1](/images/cs231n/slides/lecture13/winter1516_lecture13-001.png)

## Segmentation(intro)

![p1](/images/cs231n/slides/lecture13/winter1516_lecture13-017.png)

![p1](/images/cs231n/slides/lecture13/winter1516_lecture13-020.png)

### Semantic Segmentation

![p1](/images/cs231n/slides/lecture13/winter1516_lecture13-021.png)

- classification : one label per image
- semantic segmentation : one label per pixel

### Instance Segmentation

![p1](/images/cs231n/slides/lecture13/winter1516_lecture13-022.png)

SDS

## Semantic Segmentation

![p1](/images/cs231n/slides/lecture13/winter1516_lecture13-024.png)

![p1](/images/cs231n/slides/lecture13/winter1516_lecture13-028.png)

use patch

![p1](/images/cs231n/slides/lecture13/winter1516_lecture13-029.png)

fully convolutional network

smaller output due to pooling

### Multi-Scale

![p1](/images/cs231n/slides/lecture13/winter1516_lecture13-034.png)

1. scale to 3-size 
2. run CNN
3. upscale
4. bottom-up segmentation: 
   1. superpixels: collapse coherent
   2. tree: tie nodes
5. combine

### Refinement
#### iterative refinement

![p1](/images/cs231n/slides/lecture13/winter1516_lecture13-040.png)

1. Apply CNN (Refinement)
2. Apply CNN Again (Refinement)
3. And Again (Refinement)

more itertation more improve result

### Upsampling

![p1](/images/cs231n/slides/lecture13/winter1516_lecture13-042.png)

Long, Shelhamre, and Darrell "Fully Convolutional Networks for Semantic Segmentation", CVPR 2015

use **Learnable upsampling**

![p1](/images/cs231n/slides/lecture13/winter1516_lecture13-043.png)

upsampling

![p1](/images/cs231n/slides/lecture13/winter1516_lecture13-045.png)

skip connections = Better results

#### Deconvolution

![p1](/images/cs231n/slides/lecture13/winter1516_lecture13-046.png)

$(4-3+2\times1)/1 = 4$

![p1](/images/cs231n/slides/lecture13/winter1516_lecture13-049.png)

$(4-3+2\times1)/3 + 1 = 2$

![p1](/images/cs231n/slides/lecture13/winter1516_lecture13-052.png)

same as backward pass for normal convolution

deconv's FP = conv's BP
deconv's BP = conv's FP

![p1](/images/cs231n/slides/lecture13/winter1516_lecture13-057.png)

Better names
- convolution transpose
- backward strided convolution
- 1/2 strided convolution
- upconvolution

> Definition of Deconvolution in Signal processing
> inverse of convolution (i.e. undoing convolution)

![p1](/images/cs231n/slides/lecture13/winter1516_lecture13-059.png)

![p1](/images/cs231n/slides/lecture13/winter1516_lecture13-060.png)

## Instance Segmentation

![p1](/images/cs231n/slides/lecture13/winter1516_lecture13-062.png)

![p1](/images/cs231n/slides/lecture13/winter1516_lecture13-063.png)

![p1](/images/cs231n/slides/lecture13/winter1516_lecture13-069.png)

similar to R-CNN

### Hypercolumns

![p1](/images/cs231n/slides/lecture13/winter1516_lecture13-071.png)

