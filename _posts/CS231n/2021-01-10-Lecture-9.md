---
published: true
title: Lecture 9. Understanding and Visualizing Convolutional Neural Networks
category: [CS231n]
use_math: true
---

> 해당 포스트는 송교석 님의 유튜브 강의를 정리한 내용입니다. 강의 영상은 [여기](https://youtube.com/playlist?list=PL1Kb3QTCLIVtyOuMgyVgT-OeW0PYXl3j5)에서 보실 수 있습니다.

이번 포스트에서는 Convolutional Neural Network 를 Visualize 하는 것을 공부해보겠습니다.

`CNN`이 훌륭한 성능을 보인다는 것은 여러측면에서 살펴본 바가 있습니다만, 어떻게 그런 성능을 내는 것인지 HOW 에 대한 설명이 더 필요합니다.

그래서 이를 시각화하여 들여다보겠습니다.

## Visualizations

![p6](/images/cs231n/slides/lecture9/winter1516_lecture9-06.png)

이번 포스트에서 위와 같은 여러가지 시각화 주제에 대해서 알아볼 것인데, 우선 `CNN`이 무엇을 하는지 이해하는 가장 간단한 방법은, `CNN`의 raw activation 을 보는 것일 겁니다.

### Visualize Patches

![p7](/images/cs231n/slides/lecture9/winter1516_lecture9-07.png)

위 이미지는 하나의 neuron 을 activation 하는 부분이 어떤 부분인가를 시각화 하서 보여주는 것입니다.

예를 들어 pool5 Layer 에서 임의의 neuron 을 취한다음에 여러 이미지들을 이 Conv. Net 에 돌려줍니다. 그렇게 하면서 이 pool5 Layer 에서 추출한 임의의 neuron 을 가장 excite 시키는 것이 어떤 이미지인지를 살펴본 것 입니다.

이미지들의 배열에서 각 행이 neuron 을 의미한다고 보면 됩니다. 첫번째행의 경우 사람에게 반응한 것이라고 보면 됩니다.

이처럼 neuron 을 가장 activation 하는 부분이 어떤 부분인지를 시각화 하는 방법이 있습니다.

### Visualize Weights

![p7](/images/cs231n/slides/lecture9/winter1516_lecture9-08.png)

두 번째 방법은 filter 즉, kernel 을 visualize 하는 방법입니다.

이렇게 얻은 이미지는 마치 `gabor filter` 같이 생겼습니다. `gabor filter`는 특정 방향성에 대한 외곽선을 검출하는 것으로 texture 분석에 많이 사용됩니다.

예를 들어 conv1 의 filter 들을 시각화하게 되면 `gabor filter` 처럼 생긴 이미지를 얻게 됩니다.

그런데, 첫 번째 CONV. Layer 에 있는 필터들만 이미지에 직접 작용하게 됩니다.

그렇기 때문에, 첫 번째 레이어의 필터들만 해석이 가능하다라는 것이 되겠습니다.

![p9](/images/cs231n/slides/lecture9/winter1516_lecture9-09.png)

그 다음의 레이어들의 weight 들은 위 그림 처럼 시각화할 수는 있지만, 이 weight 들을 raw 이미지에 대한 것이 아니라, 전 단계 Layer 의 activation 에 대한 visualize 이기 때문에 해석이 용이하지 않습니다.

그래서 의미가 그렇게 크지 않은 단점이 있습니다. 위 그림에서 괄호 `()` 안의 것들이 하나의 필터에 대응하는 것이라고 보면 됩니다.

![p9](/images/cs231n/slides/lecture9/winter1516_lecture9-10.png)

사실 이런 `gabor filter` 같은 형태의 visualize 를 보면 굉장히 있어보이긴 합니다만, DeepLearning 의 CNN 에서만 만들 수 있는게 아니라 전통적으로 알고리즘기반의 여러가지 다양한 방식들에 의해서도 나타나는 이미지들 입니다.

그래서 `gabor filter` 과 같은 것들은 피로감이 있을 정도로 예전부터 보아왔다라는 것 입니다.

### Visualize Representation Space

![p11](/images/cs231n/slides/lecture9/winter1516_lecture9-11.png)

세 번째의 시각화 방법은 `Representation` 자체를 visualize 하는 것 입니다.

classification 을 수행하기 직전의 FC7. Layer 에 이미지에 대한 4096차원의 `code` 가 들어 있다고 생각할 수 있는데, 여러개의 이미지에 대한 각각의 `code`를 모아서 한번에 visualize 하는 방법입니다.

![p12](/images/cs231n/slides/lecture9/winter1516_lecture9-12.png)

대표적인 방법으로는, `t-SNE visualization` 있습니다. 여기에서는 `CNN`의 시각으로 볼 때, 유사한 것들을 가까운데로 위치시켜서 clustering 해주는 방법입니다.

오른쪽 그림은, MNIST 데이터를 `t-SNE visualization` 한 것입니다.

![p13](/images/cs231n/slides/lecture9/winter1516_lecture9-13.png)

또 다른 예시로, ImageNet 의 1000개의 class 에 대해서 CNN 의 시각으로 볼 때, 가까운 것들을 가깝게 위치시킨 것 입니다.

이것을 확대해서 보게 되면, 사람들로 구성되어 있는 부분, 시계로 구성되어있는 부분들이 모여있는 것을 확인할 수 있습니다.

### Occlusion experiments

![p15](/images/cs231n/slides/lecture9/winter1516_lecture9-15.png)

네 번째 방법으로는 Occlusion experiments 라는 것입니다.

ZFNet 을 만든 분들이 은닉, 은폐를 통해서 실험한 것으로 occluder 라는 회색부분(은페한 부분)들을 0으로 된 정사각형의 행렬로 만들어서 하나의 function 을 만든 겁니다. 즉, 이 function 에서는 occluder 들의 위치에 따라서 얼마나 이미지를 잘 분류하는지, 확률에 어떤 변화가 있는지를 마치 hit map 과 같은 방식으로 해서 보여줍니다.

occluder 를 sliding 하여 진행했을 때, 그 위치에 따른 분류 확률을 살펴본 것입니다.

예상할 수 있듯이, 파란부분에 occluder 가 위치하게 되었을때, 분류능력이 현저하게 떨어지는 것을 확인할 수 있습니다. 그리고 빨간부분에 위치하게 되면 분류능력이 상승했다고 합니다. 3번째 그림에서 사람의 얼굴을 가리게 되면 하운드로 분류할 확률이 올라갔다는 의미입니다.

### Visualize Activations

![p16](/images/cs231n/slides/lecture9/winter1516_lecture9-16.png)

이번에는 [참고영상](https://www.youtube.comwatch?v=AgkflQ4lGaM)을 통해 activation 을 시각화하는 것을 보겠습니다. 지금까지 살펴본대로 CNN 을 이용해서 이미지를 잘 분류하는 것을 학습해왔는데, 그러나 여전히 이것을 어떻게 잘 분류하느냐 즉, HOW 에 대한 부분이 여전히 black-box 로 남아있습니다. 그러다보니 이런 visualization tool 을 만들어서 보다 시각화하여 보여줌으로써 black-box 를 풀어나가는데에 도움을 주고 싶다는 내용 입니다.

참고영상에서 잠깐 언급이 되었는데, Activation 을 visualization 하는데에는 두 가지 방법이 있습니다.

1. Deconvolution-based approach
2. Optimizationi-based approach

위 두 가지 방법이 있는데, 각각에 대해서 살펴보겠습니다.

### Deconvolution based Approach

![p17](/images/cs231n/slides/lecture9/winter1516_lecture9-17.png)

이를 알아보기 앞서서 어떤 이미지가 input 으로 들어올때, 이 input 에 대한 특정 레이어에서의 하나의 뉴런의 gradient 를 계산하려면 어떻게 해야할까요?

임의의 뉴런이 위치한 레이어까지 forward pass 를 해주고 activation 을 구해준 다음에 해당 레이어에 있는 뉴런들 중 보려고 하는 graident 를 제외하고 나머지를 0으로 주어 필터링 해준다음 back propagation 을 진행해주면 됩니다.

위 처럼 진행을 하게 되면 먼저 1. 이미지를 net 에 집어넣고,

![p20](/images/cs231n/slides/lecture9/winter1516_lecture9-20.png)

레이어를 골라서 보고자 하는 gradient 를 제외하고 모두 0으로 설정하고 관심있는 neuron 만 1로 설정해줍니다.

그리고 backpropagation 을 진행합니다.

이렇게 하면 이미지에 대한 gradient 를 시각화해서 볼 수 있게 되는 것 입니다.

위 이미지를 보게 되면 blobby 한 이미지가 나오게 되는데, 그 이유는 positive 한 influence 와 negative 한 influence 가 서로 충돌하다가 서로 canceling out 을 하게 되어 애매한 이미지가 나오게 되는 것 입니다.

그래서 이것을 좀 더 선명하게 만들어주는 방법은 그냥 back propagation 을 사용하는 것이 아니라 `Guided backpropagation` 을 사용하는 것 입니다.

`Guided backpropagation` 을 사용하게 되면 positive 한 influence 만 back propagation 시에 반영합니다. 그렇게 하여 오른쪽 그림과 같은 선명한 이미지를 얻을 수 있습니다.

뒤에 좀 더 살펴보겠지만, `Guided backpropagation`은 다른 것은 바뀐게 없고 `ReLU` 대신에 `Modified ReLU` 를 이용해서 이런 결과를 얻어 낼 수 있는 것 입니다.

![p21](/images/cs231n/slides/lecture9/winter1516_lecture9-23.png)

a) 이 과정을 자세히 살펴보게 되면, input 이미지가 들어오고 forward pass 를 거쳐서 $f^L$ 과 같은 feature map 이 나왔다고 하겠습니다. 이때 우리가 관심을 가지고 있는 neuron 이 `2` 라고 한다면, 이 neuron 의 gradient 만 1로 놔두고 나머지들은 0으로 처리한 것입니다.

b) `ReLU` 의 경우를 좀 더 살펴보게 되면 input 이 들어왔을 떄, forward pass 를 진행하게 되면 0 보다 작은 빨간색 부분들을 모두 0으로 치환합니다. 왜냐하면 `ReLU`가 max 값을 취하는 형태이기 때문입니다. 그래서 backward pass 에서는 빨간색 부분은 그대로 0이 될 것이고 나머지 노란색 부분에 대해서만 backward pass 가 진행되는 것을 볼 수 있습니다.

결과적으로 4군데 0인 곳을 제외한 나머지는 그대로 전달되는 것을 확인할 수 있는데, `Guided backpropagation` 에서는 `ReLU` 에 의해 0 으로 처리된 부분 외에 마이너스(-)값을 가지는 노란색부분들이 모두 0으로 변합니다. 그래서 결과적으로 6과 3만 남게됩니다.

즉, 이 식에서도 보이다싶이 positive 한 influence 만 주는 것들만 통과를 시키는 것입니다.

![p21](/images/cs231n/slides/lecture9/winter1516_lecture9-24.png)

`Guided Backpropagation`을 한 것들의 예시를 보게되면, 위에서 차례로 `conv6`, `conv9` 레이어인데, 선명하게 나오는 것을 확인할 수 있습니다.

![p21](/images/cs231n/slides/lecture9/winter1516_lecture9-25.png)

Deconv approcaches 에서 또 하나의 방법은 `deconvnet` 이라는 것이 있습니다.

`deconvnet`라는 것도 논문에서 같이 제시됐습니다.

c) 를 보시게 되면, 조금 전의 `guided backpropagation`에는 `activation` 즉 `ReLU`가 식에 들어가 이에 의해서 0으로 되어있는 부분들은 0으로 결과가 나오는 것인데, backward `deconvnet`에서는 `ReLU` 의 영향을 받지 않기 때문에, 마이너스 값들이 있을 때 이 것을 backpropagation 을 하게 되면, 그냥 마이너스인 값들은 모두 0으로 처리가 되게 됩니다. `ReLU` 와는 관계 없이 0이면 무조건 0이 되는 것 입니다. 물론, 양수 값들은 그대로 전달되게 됩니다.

![p21](/images/cs231n/slides/lecture9/winter1516_lecture9-26.png)
![p21](/images/cs231n/slides/lecture9/winter1516_lecture9-27.png)
![p21](/images/cs231n/slides/lecture9/winter1516_lecture9-28.png)

예시 그림을 보게되면, `deconvnet` 역시 잘 동작하는 것을 볼 수 있습니다.

Layer 1, 2, ... , 5 까지 잘 Visualize 되는 것을 확인 할 수 있습니다.

지금까지 Deconvolution 의 접근 방법들에 대해서 살펴봤습니다.

Deconvolution 은 앞에서 살펴보았다싶이 한번에 forward 되고, 한번에 backward 되는 것이 장점이라고 볼 수 있지만, 이제부터 살펴보게될 optimization 기반의 접근방법은 조금 더 복잡하게 진행됩니다. 하지만 이 것 보다 직관적으로는 더 이해가 쉽습니다.

## Optimization

![p30](/images/cs231n/slides/lecture9/winter1516_lecture9-30.png)

명칭으로 보게 되면, Optimization to image 입니다. 말 그대로 이미지가 최적화의 대상이 되는 파라미터가 되는 것 입니다. 즉, 일반적으로 convolution 을 neural network 에서는 weight 들이 파라미터가 됐었는데, 이와는 다르게 여기에서는 convolution network 의 일반적인 weight 와 같은 파라미터들을 고정시켜주고, 이미지를 파라미터로 이용합니다.

정리하자면, 여기에서는 weight 의 업데이트가 아니라, 이미지를 업데이트 하는 것이다라고 생각하면 됩니다. 그래서 특정 Class 의 score 를 최대화 할 수 있는 이미지를 찾아보려고 하는데, 그때 이 식 $\text{arg max}S_c(I) - \lambda||I||^2_2$은 특정 클래스 c 에 대한 score 를 최대화 하는 인자를 찾아내면 되는 것입니다. 물론 여기에는 적절한 L2 regularization 과 같은 regularization term 이 들어가면 됩니다.

![p31](/images/cs231n/slides/lecture9/winter1516_lecture9-31.png)

일단 0으로 구성된 zero image 를 network 에 넣어주고 forward pass 를 해줍니다. 여기서 중요한 것은 score vector 에서 우리가 관심을 가지고 있는 Class 에 대해서만 1로 설정해주고 나머지는 0으로 설정을 해줍니다. 그렇게 하고 backpropagation을 진행하는 것 입니다.

이렇게 함으로써 weight 의 업데이트가 아닌 이미지에 대한 업데이트를 약간 수행하게 되는 것이고, 그 다음에는 이를 반복하면 되는 것 입니다.

![p32](/images/cs231n/slides/lecture9/winter1516_lecture9-32.png)

방금 업데이트 된 이미지를 network 에 forward pass 하고 특정 Class 만 1로 만들어주고 난 후 다시 backpropagation 을 하는 이러한 일련의 과정을 계속 반복하게 되면 다음과 같은 결과를 얻게 됩니다.

![p32](/images/cs231n/slides/lecture9/winter1516_lecture9-33.png)

덤벨, 컵, 달마시안 같은 것들이 최초에는 zero image 에서 시작하여 이 클래스가 어떤 것을 보고 activation 이 되는 지를 확인할 수 있는 것입니다.

![p32](/images/cs231n/slides/lecture9/winter1516_lecture9-34.png)

위 거위와 같은 경우는 한 마리만 있는 것이 아니라 여러마리가 곳곳에 위치해서 클래스의 스코어를 최대한 maximize 하려고 노력하고 있다는 것을 눈으로 확인할 수 있는 것 입니다.

이렇게 특정 Class 의 score 를 maximize 하는 이미지를 찾아내는 방법을 알아보았습니다.

다음으로, data 의 gradient 를 visualize 하는 방법을 보겠습니다.

## Visualize Data Gradient

![p35](/images/cs231n/slides/lecture9/winter1516_lecture9-35.png)

기본적으로 data 의 gradient 는 3개의 채널을 갖습니다. 이를 염두에 두고 ($i, j, c$) 세 개의 채널에 대해서 squish 해 줌으로써 어떤 결과가 나오는 지를 볼 수 있습니다.

그 방법은 다음과 같습니다. 예를 들어 강아지 이미지가 있다고 할 떄, 이를 일단 Convolutional Network 에 돌리고 강아지의 gradient 를 1로 설정해줍니다. 그 상태에서 backpropagation 을 하면 이미지 gradient 에 도달했을때, 그 순간의 RGB channel 을
squish 해버립니다. 이렇게 되면 1차원의 hit map 같은 것들이 생성되게 되게 됩니다. 그 그림은 다음과 같이 됩니다.

![p36](/images/cs231n/slides/lecture9/winter1516_lecture9-36.png)

위와 같이 object 가 있는 부분을 1차원의 hit map 같은 형태로 표현한 것을 볼 수 있습니다.
물론, 하얀부분이 영향을 주는 부분이고 검은 부분이 영향을 주지 못하는 부분이 되겠습니다. 각각의 픽셀들의 영향력의 강도, 세기를 나타내는 것이라고 볼 수 있을 것 입니다. 이런 방식을 활용한 `grabcut` 이라는 segmentation 알고리즘이 있습니다.

![p37](/images/cs231n/slides/lecture9/winter1516_lecture9-37.png)

이 `grabcut`을 이용해서 segmentation 을 한 사레도 있다고 합니다. 원본이미지를 gradient 로 표현해서 `grabcut`을 이용하여 segmentation 을 하는 것 입니다.

지금 살펴본 것은 우리가 원하는 Class 의 score 의 gradient 를 1로 주어 해당 Class 를 시각화 했습니다.

![p38](/images/cs231n/slides/lecture9/winter1516_lecture9-38.png)

하지만 마지막 레이어에서 score 의 gradient 를 1로 주지 않고, 이를 ConvNet의 과정에서 어떤 layer 든 간에 임의의 뉴런에 대해서 이를 똑같은 방법으로 실행할 수 있습니다.

방법은 동일한데 2번에서, 어떤 특정 레이어에서 우리가 관심을 가지고 있는 그 뉴런의 activation 값을 1로 해주고 나머지 뉴런들의 activation 은 모두 0으로 만들어 주는 것입니다.

이렇게 함으로써 동일한 효과를 얻을 수 있다는 것입니다.

![p39](/images/cs231n/slides/lecture9/winter1516_lecture9-39.png)

보면 앞에서 봤던 것은 기본적으로 score 애 대해서 빨간색 박스와 같은 regularization term 을 사용했었습니다. 그런데 이 논문에서 제시하는 것은 동일한 방법으로 이미지를 업데이트 시키는데 위의 regularization term 대신에 이미지 자체를 blur 해준다는 것입니다.

blur 를 함으로써 빈도 높게 발생하는 것을 방지하기 때문에, 이것이 오히려 효과가 더 좋다는 것입니다.

![p40](/images/cs231n/slides/lecture9/winter1516_lecture9-40.png)

그 결과는 위와 같이 앞에서 본 것들 보다는 조금 더 선명하게 나오는 것을 확인할 수 있습니다.

여기에서 차이점은 앞의 과정에서는 L2 regularization 을 사용하였고, 지금 과정에서는 단지 blur 처리를 해줬다는 것 외에는 차이가 없습니다.

마지막 레이어에서는 플라밍고나 펠리컨 같은 것들이 보이는 것이고, 전 단계의 레이어들은 다음과 같이 됩니다.

![p41](/images/cs231n/slides/lecture9/winter1516_lecture9-41.png)
![p41](/images/cs231n/slides/lecture9/winter1516_lecture9-42.png)
![p41](/images/cs231n/slides/lecture9/winter1516_lecture9-43.png)

이렇게 보이게 됩니다. 여기서 생각해야 되는 것은 receptive field 가 Layer 2, 3 같은 데에서는 굉장히 작아, 좁은 영역을 커버하는 것이고, 뒷단으로 갈 수록 field 가 넓어지면서 이미지 전체를 커버하기 때문에 좀 더 추상적이고 blobby 한 이미지가 나타나게 되는 것입니다.

![p44](/images/cs231n/slides/lecture9/winter1516_lecture9-44.png)

다음으로 넘어가기전에, CNN의 코드가 주어졌을 떄, 원본 이미지를 복원할 수 있겠는가라는 물음에 대해 생각해보겠습니다.

예를 들어, 위 그림에서 빨간색 박스처리 되어있는 FC 7 레이어에서는 4096 차원의 벡터이기 떄문에 4096개의 코드를 가집니다. 이것으로 부터 원본이미지를 복원할 수 있겠는가라는 것 입니다.

> 복원한다는 것으로 reconstruct 나 invert 한다는 표현을 많이 사용합니다.

![p42](/images/cs231n/slides/lecture9/winter1516_lecture9-45.png)

복원해야 하는 이미지는 주어진 코드와 유사할 것이고, 자연스러워 보여야 합니다.

위 식은 regression 을 진행하면서 차이가 가장 유사한 이미지를 찾는 과정이 되겠습니다.

여기서 임의의 feature 를 maximize 하는 것이 아니라, 특정 feature 를 maximize 하여 이를 찾게 되는 것입니다.

실례를 보면 다음과 같습니다.

![p46](/images/cs231n/slides/lecture9/winter1516_lecture9-46.png)

앞에서 설명한 것 처럼 4096개의 코드로 이를 복원한 것 입니다. 그림이 여러개인 것은 각각 initialization 을 다르게 한 것입니다. 위 그림을 보게 되면, 4096개의 코드가 이미지를 복원하기 위한 어느정도의 정보를 가지고 있는지 짐작해 볼 수 있습니다.

![p47](/images/cs231n/slides/lecture9/winter1516_lecture9-47.png)

이러한 작업은 비단, FC 7 레이어에서만 가능한 것이 아니라 Convolution Network 의 그 어떤 것도 모두 가능합니다. 예를들어 위의 이미지들은 마지막 pooling 레이어의 representation 으로 부터 복원한 것인데, FC 7 레이어보다는 더 앞단에 있기 때문에 spacial location 에 대한 정보가 좀 더 정확하게 나타나 있는 것을 확인할 수 있습니다.

![p48](/images/cs231n/slides/lecture9/winter1516_lecture9-48.png)

위 그림은 모든 레이어를 단일 이미지가 처리되면서 어떻게 보이는지를 나타내는데, 뒷단으로 갈 수록 blobby 해지고, 앞단으로 갈 수록 좀 더 선명해지는 것을 확인할 수 있습니다.

그리고 forward pass 를 진행하면서 원본 이미지에 대한 정보를 어떻게 잃어가고 있는지를 대강 파악할 수 있을 것입니다.

![p49](/images/cs231n/slides/lecture9/winter1516_lecture9-49.png)

이 또한 앞에서 본 것과 비슷합니다.

![p50](/images/cs231n/slides/lecture9/winter1516_lecture9-50.png)

위 그림들은 구글에서 발표한 Deep Dream 의 결과들인데, 기본적으로 Deep Dream 도 지금까지 본 이미지에 대한 optimization 기법을 그대로 활용합니다.

![p51](/images/cs231n/slides/lecture9/winter1516_lecture9-51.png)

코드도 100줄 정도 밖에 안되는 정도로 구현이 되는데, 여기서 코드 일부분만 한번 보겠습니다.

이미지를 업데이트할 때마다 위의 `make_step` 이라는 함수를 호출하게 됩니다. 파라미터 중 `end` 라는 것에 실제로 어떤 레이어에서 위의 Deep Dreaming 을 할 것인가를 지정하게 되어있습니다.

![p52](/images/cs231n/slides/lecture9/winter1516_lecture9-52.png)

주요부분을 보면, regularizer 를 jitter 하는 것도 있지만, 핵심은 그 사이의 빨간 박스 부분입니다.

`net.forward` 를 해서 딥드립을 하길 원하는 레이어까지 forward pass 를 해주고 `dst`  를 인자로하여 `objective` 함수를 호출하는데, `dst`는 위의 코드 `dst = net.blobs[end]` 를 보면 `blobs` 이라는 것을 알 수 있습니다. caffe 에서 이 `blob`을 사용하는데 이는 두 가지 종류가 있습니다.

`blob`

- diff field : gradient 정보
- data field : raw activation 정보

위 두가지 정보를 가지고 있는 `dst`를 인자로 넘겨주어, `objective` 라는 함수에서 gradient 를 activation 으로 설정해줍니다. 즉, `dx = x` 로 설정한다는 것이 핵심이 되겠습니다.

이후, `net.backward`로 backward pass 를 해주고 normalization 과 image update, clipping 을 진행합니다.

앞에서 핵심은 gradient 값을 activation 으로 그대로 설정해준다는 것이라고 했는데, 이게 어떤 의미인지를 생각해 볼 필요가 있습니다. 여기서 activation 으로 `ReLU`를 사용하고 있습니다. 이 `ReLU`의 특성은 activation이 zero cropped 된다는 것입니다. 음수의 경우 무조건 0이 되고, 양수의 경우 그대로 값을 유지하기 떄문에 무조건 0 이상인 activation 을 가지는 상태에서 gradient 를 activation 으로 설정했습니다. 그런데 앞에서 이미지에 대한 optimization 을 할때, 특정 class 의 score 나 특정 conv layer 에서의 activation 즉, 관심있는 뉴런의 값을 1로 주고 나머지 뉴런, gradient 들은 모두 0으로 설정했었습니다. 그런데 여기는 0으로 설정하는 것이 아니라 gradient 를 activation 값으로 설정하기 떄문에, 모든 activation 에 있어서 boosting 이 일어나게 됩니다.

![p50](/images/cs231n/slides/lecture9/winter1516_lecture9-53.png)

예를 들어, 구름이 흘러가면서 동물의 형상을 띄기도 합니다.

dog detector 가 있다고 할때, 해당 detector 가 위 그림을 보고 어떤 부분이 "개" 같이 생겼다라고 판단하면, 해당 특징을 갖는 class 가 boost 되는 효과가 일어나는 것 입니다.

그래서 오른쪽 그림과 같이 여러가지 이미지들이 그림과 함께 boost 되어 나타나는 것을 볼 수 있는데, "개" 가 많이 나오게 됩니다. 그 이유는 ImageNet 의 1000개의 Class 중 200개 정도가 개와 관련된 Class 이기 때문입니다.

![p54](/images/cs231n/slides/lecture9/winter1516_lecture9-54.png)

그래서 이처럼 다양하고 이상한 이미지를 생성하는 것이 Google 의 Deep Dream 이 되겠습니다.

![p54](/images/cs231n/slides/lecture9/winter1516_lecture9-55.png)

위의 레이어는 3b 레이어로 앞에서 봤던 4c 레이어 보다는 앞단에 있는 레이어이기 때문에 구체적인 형상이 아니라 패턴위주로 boosting 이 일어난 것을 볼 수 있습니다.

다시 한번 핵심을 짚고 넘어가면, Google 의 Deep Dream 은 gradient 를 activation 값으로 설정하는 것이 핵심이고, 해당 activation 이 `ReLU` 이기 때문에, boosting 효과가 일어나서 여러가지 그림들이 합성하는 것과 같은 효과가 나타난다는 것입니다.
